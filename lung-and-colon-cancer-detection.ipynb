{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1079953,"sourceType":"datasetVersion","datasetId":601280},{"sourceId":7992171,"sourceType":"datasetVersion","datasetId":4705132},{"sourceId":8036581,"sourceType":"datasetVersion","datasetId":4737682}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML Model","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"#sys libs\nimport os\nimport itertools\nimport shutil\nimport pathlib\nfrom PIL import Image\n\n#data handling tools\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nsns.set_style('whitegrid')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix , classification_report\n\n#deep learning libs\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Conv2D , MaxPooling2D , Flatten , Activation , Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam , Adamax\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n#IPYwidgets\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport io\nfrom io import BytesIO\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:37:16.748842Z","iopub.execute_input":"2024-05-05T02:37:16.749537Z","iopub.status.idle":"2024-05-05T02:37:16.758321Z","shell.execute_reply.started":"2024-05-05T02:37:16.749503Z","shell.execute_reply":"2024-05-05T02:37:16.757284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Read Data","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set'\nfilepaths = []\nlabels = []\n\nfolds = os.listdir(data_dir)\n\nfor fold in folds:\n    foldpath = os.path.join(data_dir, fold)\n    flist = os.listdir(foldpath)\n    \n    for f in flist:\n        f_path = os.path.join(foldpath , f)\n        filelist = os.listdir(f_path)\n        \n        for file in filelist:\n            fpath = os.path.join(f_path , file)\n            filepaths.append(fpath)\n            \n            if f == 'colon_aca':\n                labels.append('Colon adenocarcinoma')\n            \n            elif f == 'colon_n':\n                labels.append('Colon Benign Tissue')\n                \n            elif f == 'lung_aca':\n                labels.append('Lung adenocarcinoma')\n            \n            elif f == 'lung_n':\n                labels.append('Lung Benign Tissue')\n                \n            elif f == 'lung_scc':\n                labels.append('Lung Squamous Cell Carcinoma')\n                \nFiles = pd.Series(filepaths , name= 'filepaths')\nlabels = pd.Series(labels ,name = 'labels' )\ndf = pd.concat([Files , labels] , axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:37:24.073934Z","iopub.execute_input":"2024-05-05T02:37:24.074300Z","iopub.status.idle":"2024-05-05T02:37:26.421669Z","shell.execute_reply.started":"2024-05-05T02:37:24.074270Z","shell.execute_reply":"2024-05-05T02:37:26.420868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:37:29.423621Z","iopub.execute_input":"2024-05-05T02:37:29.423985Z","iopub.status.idle":"2024-05-05T02:37:29.445115Z","shell.execute_reply.started":"2024-05-05T02:37:29.423955Z","shell.execute_reply":"2024-05-05T02:37:29.443978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data into train , validation and test","metadata":{}},{"cell_type":"code","source":"#Split data into train 80%\ntrain , dummy = train_test_split(df , train_size = 0.8 , shuffle = True , random_state= 40 , stratify= df['labels'])\n#Split data into validation and test 10% both\nvalid , test = train_test_split(dummy , train_size = 0.5 , shuffle = True , random_state= 40 , stratify= dummy['labels'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:37:33.473516Z","iopub.execute_input":"2024-05-05T02:37:33.473855Z","iopub.status.idle":"2024-05-05T02:37:33.524053Z","shell.execute_reply.started":"2024-05-05T02:37:33.473829Z","shell.execute_reply":"2024-05-05T02:37:33.523295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Data Generator","metadata":{}},{"cell_type":"code","source":"tr_gen = ImageDataGenerator()\nts_gen = ImageDataGenerator()\n\nimg_size = (224 , 224)\n\ntrain_gen = tr_gen.flow_from_dataframe(train , x_col = 'filepaths' , y_col = 'labels' ,target_size = img_size ,\n                                       class_mode= 'categorical',color_mode = 'rgb' , shuffle= True , batch_size= 16)\n\nvalid_gen = ts_gen.flow_from_dataframe(valid , x_col = 'filepaths' , y_col = 'labels' , target_size = img_size ,\n                                      class_mode = 'categorical' , color_mode= 'rgb' , shuffle = True , batch_size= 16)\ntest_gen = ts_gen.flow_from_dataframe(test , x_col = 'filepaths' , y_col = 'labels' , target_size = img_size ,\n                                     class_mode = 'categorical' , color_mode = 'rgb' , shuffle= False , batch_size = 16)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:37:37.684020Z","iopub.execute_input":"2024-05-05T02:37:37.684404Z","iopub.status.idle":"2024-05-05T02:38:38.627084Z","shell.execute_reply.started":"2024-05-05T02:37:37.684375Z","shell.execute_reply":"2024-05-05T02:38:38.626238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample from train data","metadata":{}},{"cell_type":"code","source":"dict_gen = train_gen.class_indices # defines dictionary {'class': index}\nclasses = list(dict_gen.keys()) #define list of dictionary that have the names of the classes in the train data\nimages , labels = next(train_gen) #get batch size sample from train generator \n\nplt.figure(figsize= (20,20))\n\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    image = images[i] / 255 # scales data to range (0 - 255)\n    plt.imshow(image)\n    index = np.argmax(labels[i]) # get image index\n    class_name = classes[index] # get class of image\n    plt.title(class_name , color= 'blue' , fontsize = 12)\n    plt.axis('off')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:38:42.623539Z","iopub.execute_input":"2024-05-05T02:38:42.623856Z","iopub.status.idle":"2024-05-05T02:38:47.092224Z","shell.execute_reply.started":"2024-05-05T02:38:42.623832Z","shell.execute_reply":"2024-05-05T02:38:47.090965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Structure","metadata":{}},{"cell_type":"code","source":"# Create Model Structure\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\nclass_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n\n# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n# we will use efficientnetv2b3 from EfficientNet family.\nbase_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n# base_model.trainable = False\n\nmodel = Sequential([\n    base_model,\n    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n    Dropout(rate= 0.45, seed= 123),\n    Dense(class_count, activation= 'softmax')\n])\n\nmodel.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:38:54.260107Z","iopub.execute_input":"2024-05-05T02:38:54.260515Z","iopub.status.idle":"2024-05-05T02:39:01.006373Z","shell.execute_reply.started":"2024-05-05T02:38:54.260483Z","shell.execute_reply":"2024-05-05T02:39:01.005449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:39:05.294342Z","iopub.execute_input":"2024-05-05T02:39:05.294691Z","iopub.status.idle":"2024-05-05T02:39:05.512075Z","shell.execute_reply.started":"2024-05-05T02:39:05.294664Z","shell.execute_reply":"2024-05-05T02:39:05.511175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Iterate","metadata":{}},{"cell_type":"code","source":"epochs = 10\nhistory = model.fit(x= train_gen , epochs= epochs , validation_data= valid_gen , validation_steps=None , shuffle = False , verbose= 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:40:25.179349Z","iopub.execute_input":"2024-05-05T02:40:25.179714Z","iopub.status.idle":"2024-05-05T03:21:12.021027Z","shell.execute_reply.started":"2024-05-05T02:40:25.179685Z","shell.execute_reply":"2024-05-05T03:21:12.020118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Performance","metadata":{}},{"cell_type":"code","source":"# Define needed variables\ntr_acc = history.history['accuracy']\ntr_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nindex_loss = np.argmin(val_loss)\nval_lowest = val_loss[index_loss]\nindex_acc = np.argmax(val_acc)\nacc_highest = val_acc[index_acc]\nEpochs = [i+1 for i in range(len(tr_acc))]\nloss_label = f'best epoch= {str(index_loss + 1)}'\nacc_label = f'best epoch= {str(index_acc + 1)}'\n\n# Plot training history\nplt.figure(figsize= (20, 8))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\nplt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\nplt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\nplt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\nplt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:21:29.655433Z","iopub.execute_input":"2024-05-05T03:21:29.656141Z","iopub.status.idle":"2024-05-05T03:21:30.409019Z","shell.execute_reply.started":"2024-05-05T03:21:29.656097Z","shell.execute_reply":"2024-05-05T03:21:30.408007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"ts_length = len(test)\ntest_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\ntest_steps = ts_length // test_batch_size\n\ntrain_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\nvalid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\ntest_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n\nprint(\"Train Loss: \", train_score[0])\nprint(\"Train Accuracy: \", train_score[1])\nprint('-' * 20)\nprint(\"Valid Loss: \", valid_score[0])\nprint(\"Valid Accuracy: \", valid_score[1])\nprint('-' * 20)\nprint(\"Test Loss: \", test_score[0])\nprint(\"Test Accuracy: \", test_score[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:21:35.204094Z","iopub.execute_input":"2024-05-05T03:21:35.204919Z","iopub.status.idle":"2024-05-05T03:21:52.584031Z","shell.execute_reply.started":"2024-05-05T03:21:35.204886Z","shell.execute_reply":"2024-05-05T03:21:52.583105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Prediction","metadata":{}},{"cell_type":"code","source":"preds = model.predict_generator(test_gen)\ny_pred = np.argmax(preds , axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:36:53.694778Z","iopub.execute_input":"2024-05-05T03:36:53.695710Z","iopub.status.idle":"2024-05-05T03:36:54.206113Z","shell.execute_reply.started":"2024-05-05T03:36:53.695673Z","shell.execute_reply":"2024-05-05T03:36:54.204331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix and Classification Report","metadata":{}},{"cell_type":"code","source":"g_dict = test_gen.class_indices\nclasses = list(g_dict.keys())\n\n# Confusion matrix\ncm = confusion_matrix(test_gen.classes, y_pred)\n\nplt.figure(figsize= (10, 10))\nplt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation= 45)\nplt.yticks(tick_marks, classes)\n\n\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n\nplt.tight_layout()\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:22:55.830601Z","iopub.execute_input":"2024-05-05T03:22:55.831280Z","iopub.status.idle":"2024-05-05T03:22:56.404942Z","shell.execute_reply.started":"2024-05-05T03:22:55.831237Z","shell.execute_reply":"2024-05-05T03:22:56.403868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification report\nprint(classification_report(test_gen.classes, y_pred, target_names= classes))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:23:01.019039Z","iopub.execute_input":"2024-05-05T03:23:01.019427Z","iopub.status.idle":"2024-05-05T03:23:01.039461Z","shell.execute_reply.started":"2024-05-05T03:23:01.019395Z","shell.execute_reply":"2024-05-05T03:23:01.038430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the model\nmodel.save('Model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:23:04.724085Z","iopub.execute_input":"2024-05-05T03:23:04.724827Z","iopub.status.idle":"2024-05-05T03:23:06.047351Z","shell.execute_reply.started":"2024-05-05T03:23:04.724793Z","shell.execute_reply":"2024-05-05T03:23:06.046465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction using loaded_model","metadata":{}},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model('/kaggle/working/Model.h5', compile=False)\nloaded_model.compile(Adam(learning_rate= 0.0001) , loss = 'categorical_crossentropy' , metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:37:05.659898Z","iopub.execute_input":"2024-05-05T03:37:05.660270Z","iopub.status.idle":"2024-05-05T03:37:10.663966Z","shell.execute_reply.started":"2024-05-05T03:37:05.660241Z","shell.execute_reply":"2024-05-05T03:37:10.663151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the widget\nimageFile = widgets.FileUpload(\n    accept='.jpeg',   # The type of files to accept\n    multiple=False    # Whether to allow to upload multiple files or not\n)\n\n# Display the widget\ndisplay(imageFile)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:16.043894Z","iopub.execute_input":"2024-05-05T03:40:16.044810Z","iopub.status.idle":"2024-05-05T03:40:16.054542Z","shell.execute_reply.started":"2024-05-05T03:40:16.044773Z","shell.execute_reply":"2024-05-05T03:40:16.053512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, file_info in imageFile.value.items():\n    image = Image.open(io.BytesIO(file_info['content']))\n\n# Preprocess the image\nimg = image.resize((224, 224))\nimg_array = tf.keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0)\n\n# Make predictions\npredictions = loaded_model.predict(img_array)\nclasses = list(dict_gen.keys())\nclass_labels = classes\nscore = tf.nn.softmax(predictions[0])\noutput = class_labels[tf.argmax(score)]\nprint(f\"{class_labels[tf.argmax(score)]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:25.559167Z","iopub.execute_input":"2024-05-05T03:40:25.559528Z","iopub.status.idle":"2024-05-05T03:40:25.658844Z","shell.execute_reply.started":"2024-05-05T03:40:25.559499Z","shell.execute_reply":"2024-05-05T03:40:25.657971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if output == \"Colon adenocarcinoma\":\n    intent_filename = \"colon_aca_intents\"\nelif output == \"Lung adenocarcinoma\":\n    intent_filename = \"lung_aca_intents\"\nelif output == \"Lung Squamous Cell Carcinoma\":\n    intent_filename = \"lung_scc_intents\"\nelse:\n    intent_filename = \"no_cancer_intents\"","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:31.774840Z","iopub.execute_input":"2024-05-05T03:40:31.775660Z","iopub.status.idle":"2024-05-05T03:40:31.780467Z","shell.execute_reply.started":"2024-05-05T03:40:31.775622Z","shell.execute_reply":"2024-05-05T03:40:31.779494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP Model","metadata":{}},{"cell_type":"code","source":"# import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport json\nimport pickle\n\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\nimport random\nfrom keras.models import load_model\n\n# create an object of WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# importing the GL Bot corpus file for pre-processing\n\nwords=[]\nclasses = []\ndocuments = []\nignore_words = ['?', '!']\nfilepath = f\"/kaggle/input/intents/{intent_filename}.json\"\ndata_file = open(filepath).read()\nintents = json.loads(data_file)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:35.809768Z","iopub.execute_input":"2024-05-05T03:40:35.810148Z","iopub.status.idle":"2024-05-05T03:40:35.870621Z","shell.execute_reply.started":"2024-05-05T03:40:35.810104Z","shell.execute_reply":"2024-05-05T03:40:35.869512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing the json data\n# tokenization\nnltk.download('punkt')\nnltk.download('wordnet')\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n\n        #tokenize each word\n        w = nltk.word_tokenize(pattern)\n        words.extend(w)\n        #add documents in the corpus\n        documents.append((w, intent['tag']))\n\n        # add to our classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:39.314198Z","iopub.execute_input":"2024-05-05T03:40:39.314539Z","iopub.status.idle":"2024-05-05T03:40:39.333640Z","shell.execute_reply.started":"2024-05-05T03:40:39.314512Z","shell.execute_reply":"2024-05-05T03:40:39.332745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatize, lower each word and remove duplicates\n\nwords = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\nwords = sorted(list(set(words)))\n\n# sort classes\nclasses = sorted(list(set(classes)))\n\n# documents = combination between patterns and intents\nprint (len(documents), \"documents\")\n\n# classes = intents\nprint (len(classes), \"classes\", classes)\n\n# words = all words, vocabulary\nprint (len(words), \"unique lemmatized words\", words)\n\n# creating a pickle file to store the Python objects which we will use while predicting\npickle.dump(words,open('words.pkl','wb')) \npickle.dump(classes,open('classes.pkl','wb'))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:43.624238Z","iopub.execute_input":"2024-05-05T03:40:43.625116Z","iopub.status.idle":"2024-05-05T03:40:43.638070Z","shell.execute_reply.started":"2024-05-05T03:40:43.625085Z","shell.execute_reply":"2024-05-05T03:40:43.637189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create our training data\ntraining = []\n\n# create an empty array for our output\noutput_empty = [0] * len(classes)\n\n# training set, bag of words for each sentence\nfor doc in documents:\n    # initialize our bag of words\n    bag = []\n    # list of tokenized words for the pattern\n    pattern_words = doc[0]\n   \n    # lemmatize each word - create base word, in attempt to represent related words\n    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n    \n    # create our bag of words array with 1, if word match found in current pattern\n    for w in words:\n        bag.append(1) if w in pattern_words else bag.append(0)\n    # output is a '0' for each tag and '1' for current tag (for each pattern)\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n    training.append([bag, output_row])\n\n# shuffle features and converting it into numpy arrays\nrandom.shuffle(training)\ntraining = np.array(training, dtype=\"object\")\n\n# create train and test lists\ntrain_x = list(training[:,0])\ntrain_y = list(training[:,1])\n\nprint(\"Training data created\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:49.390023Z","iopub.execute_input":"2024-05-05T03:40:49.391106Z","iopub.status.idle":"2024-05-05T03:40:49.410673Z","shell.execute_reply.started":"2024-05-05T03:40:49.391061Z","shell.execute_reply":"2024-05-05T03:40:49.409692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create NN model to predict the responses\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(train_y[0]), activation='softmax'))\n\n# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\nsgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n#fitting and saving the model \nhist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=5, verbose=1)\nmodel.save('chatbot.h5', hist) # we will pickle this model to use in the future\nprint(\"\\n\")\nprint(\"*\"*50)\nprint(\"\\nModel Created Successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:40:52.839663Z","iopub.execute_input":"2024-05-05T03:40:52.840413Z","iopub.status.idle":"2024-05-05T03:40:58.414472Z","shell.execute_reply.started":"2024-05-05T03:40:52.840381Z","shell.execute_reply":"2024-05-05T03:40:58.413415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the saved model file\nmodel = load_model('chatbot.h5')\nintents = json.loads(open(filepath).read())\nwords = pickle.load(open('words.pkl','rb'))\nclasses = pickle.load(open('classes.pkl','rb'))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:41:01.394029Z","iopub.execute_input":"2024-05-05T03:41:01.394767Z","iopub.status.idle":"2024-05-05T03:41:01.476098Z","shell.execute_reply.started":"2024-05-05T03:41:01.394734Z","shell.execute_reply":"2024-05-05T03:41:01.475199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_up_sentence(sentence):\n\n    # tokenize the pattern - split words into array\n    sentence_words = nltk.word_tokenize(sentence)\n    \n    # stem each word - create short form for word\n    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n    return sentence_words\n\n\n# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n\ndef bow(sentence, words, show_details=True):\n\n    # tokenize the pattern\n    sentence_words = clean_up_sentence(sentence)\n\n    # bag of words - matrix of N words, vocabulary matrix\n    bag = [0]*len(words) \n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s: \n               \n                # assign 1 if current word is in the vocabulary position\n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % w)\n    return(np.array(bag))\n\ndef predict_class(sentence, model):\n   \n    # filter out predictions below a threshold\n    p = bow(sentence, words,show_details=False)\n    res = model.predict(np.array([p]))[0]\n    error = 0.25\n    results = [[i,r] for i,r in enumerate(res) if r>error]\n    \n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    \n    for r in results:\n        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n    return return_list\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:41:04.879073Z","iopub.execute_input":"2024-05-05T03:41:04.879446Z","iopub.status.idle":"2024-05-05T03:41:04.890056Z","shell.execute_reply.started":"2024-05-05T03:41:04.879417Z","shell.execute_reply":"2024-05-05T03:41:04.889039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get the response from the model\n\ndef getResponse(ints, intents_json):\n    tag = ints[0]['intent']\n    list_of_intents = intents_json['intents']\n    for i in list_of_intents:\n        if(i['tag']== tag):\n            result = random.choice(i['responses'])\n            break\n    return result\n\n# function to predict the class and get the response\n\ndef chatbot_response(text):\n    ints = predict_class(text, model)\n    res = getResponse(ints, intents)\n    return res","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:41:09.094265Z","iopub.execute_input":"2024-05-05T03:41:09.094647Z","iopub.status.idle":"2024-05-05T03:41:09.101409Z","shell.execute_reply.started":"2024-05-05T03:41:09.094616Z","shell.execute_reply":"2024-05-05T03:41:09.100339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to start the chat bot which will continue till the user type 'end'\n\ndef start_chat():\n    print(\"Bot: Your Personal Assistant is ready.\\n\\n\")\n    while True:\n        inp = str(input()).lower()\n        if inp.lower()==\"end\":\n            break\n        if inp.lower()== '' or inp.lower()== '*':\n            print('Please re-phrase your query!')\n            print(\"-\"*50)\n        else:\n            print(f\"Bot: {chatbot_response(inp)}\"+'\\n')\n            print(\"-\"*50)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:41:12.124667Z","iopub.execute_input":"2024-05-05T03:41:12.125429Z","iopub.status.idle":"2024-05-05T03:41:12.131510Z","shell.execute_reply.started":"2024-05-05T03:41:12.125389Z","shell.execute_reply":"2024-05-05T03:41:12.130404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start the chat bot\nstart_chat()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:41:14.694015Z","iopub.execute_input":"2024-05-05T03:41:14.694908Z","iopub.status.idle":"2024-05-05T03:41:22.331226Z","shell.execute_reply.started":"2024-05-05T03:41:14.694867Z","shell.execute_reply":"2024-05-05T03:41:22.330414Z"},"trusted":true},"execution_count":null,"outputs":[]}]}